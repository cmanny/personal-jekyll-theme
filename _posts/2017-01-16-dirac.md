---
layout: post
section-type: post
title: Dirac Notation
category: tech
tags: [ 'quantum' ]
---


Dirac notation is a very elegant way to express
the state of quantum systems. For the inexperienced reader
the notation might seem intimidating, however
really it is quite simple as I
shall explain in this post. Notation in general
is only a means to an end, but if the notation
is really intuitive it can certainly speed up
problem solving. If you like this idea, you will
definitely enjoy 3Blue1Brown's video on the [Triangle
of Power](https://www.youtube.com/watch?v=sULa9Lc4pck).
I assume you know some basics of linear algebra too,
if not, 3Blue1Brown also has some very good videos on that topic.

Dirac notation is pretty much exclusively used
in quantum mechanics and quantum information. Alternatively
 called bra-ket notation, as it uses the
 $\bra{\text{bra}}$ and the $\ket{\text{ket}}$. The ket
 is used to represent a vector, which may be a basis
 vector or a linear combination of them. Here are some
 examples:

 <div>
   \begin{align}
      \ket{0} &=
        \begin{bmatrix}
         1 \\
         0
        \end{bmatrix},
      &
      \ket{1} &=
        \begin{bmatrix}
         0 \\
         1
        \end{bmatrix},
      &
      \ket{x} &= \ket{0} - \ket{1} =
        \begin{bmatrix}
        1 \\
        -1
        \end{bmatrix}
   \end{align}
 </div>

 Above we have assumed the *computational basis*, which is
 standard in quantum information texts. Now we turn our
 attention to the *bra*, which is the *conjugate transpose*
 of the ket, and vice versa. Below are the previous examples but as bras:

 <div>
   \begin{align}
      \bra{0} &=
        \begin{bmatrix}
         1 &
         0
        \end{bmatrix},
      &
      \bra{1} &=
        \begin{bmatrix}
         0 &
         1
        \end{bmatrix},
      &
      \bra{x} &= \bra{0} - \bra{1} =
        \begin{bmatrix}
        1 &
        -1
        \end{bmatrix}
   \end{align}
 </div>

There are times when thinking in terms of bras and kets
enables a speedup when manipulating expressions, however there
are also times when it is necessary to return to vectors
and matrices, as we will discover soon. It's also worth noting that
this material can be more rigorously defined, however you
can simply read Wikipedia if you prefer that.

Now that we've defined the two basic blocks of Dirac notation,
we can begin to combine them. Consider the expression $\inner{0}{1}$,
we know that in vector notation this is:

<div>
  \begin{align}
     \inner{0}{1} &=
       \begin{bmatrix}
        1 &
        0
       \end{bmatrix}
       \begin{bmatrix}
       0 \\
       1
       \end{bmatrix}

       = 0
  \end{align}
</div>

So $\inner{x}{y}$ computes the *inner product* between $\ket{x}$ and $\ket{y}$,
in this case $\inner{0}{1}$ is certainly zero since they are basis states, and
therefore orthogonal. In general for any basis states $\ket{i}$ and $\ket{j}$, we have
$\inner{i}{j}$ = $\delta_{j,i}$. Then we can generalise further so that:

<div>
  \begin{align}
     \ket{x} &= \sum_i^n x_i\ket{i} \\
     \ket{y} &= \sum_j^m y_j\ket{j} \\
     \inner{x}{y} &= \sum_i^n x_i^{*}\bra{i}\sum_j^m y_j\ket{j}
                   = \sum_i^n\sum_j^m x_i^{*}y_j\delta_{i,j}
  \end{align}
</div>

So now it should be clear how expressions can be reduced easily since
inner products between orthogonal states are always zero, and we simply perform
pattern matching to see which states are the same and retain their values.
Since in this space we have constructed an inner product, we might ask
whether there is a corresponding *outer product*? Well there is, and it's
even more elegant than the inner product. Inner products produce single
(complex) values, whereas outer products leave us with an *operator*.
Take a look at the following syntax:

<div>
  \begin{align}
     \ket{0}\bra{1} &=
     \begin{bmatrix}
      1 \\
      0
     \end{bmatrix}
     \begin{bmatrix}
      0 & 1
     \end{bmatrix}
     =
     \begin{bmatrix}
      0 & 1 \\
      0 & 0 \\
     \end{bmatrix}
  \end{align}
</div>

The operator we have made has a matrix representation, but when we use
outer products we don't need to think about matricies. Here's one way
to think about it: take the example of $\ket{0}\bra{1}$, when we
apply this operator to any state $\ket{\phi}$, it will map the state $\ket{1}$
in $\ket{\phi}$ to the state $\ket{0}$. If the state $\ket{1}$ does not exist
in $\ket{\phi}$ the result of applying the operator is 0. In general,
the operator $\ket{x}\bra{y}$ maps the basis states
in $\ket{y}$ to each basis state in $\ket{x}$. Algebraicly:

<div>
  \begin{align}
     \ket{x}\bra{y} &= \Big(\sum_i^n x_i\ket{i}\Big) \Big(\sum_j^my_j^{*}\bra{j} \Big)
                     = \sum_{i}^n\sum_{j}^m x_iy_j^{*}\ket{i}\bra{j}
  \end{align}
</div>

Here is an example to help solidify the concept:

<div>
  \begin{align}
     \ket{z} &= \ket{0} + \ket{1} \\
     \ket{w} &= \ket{0} \\
     \ket{\phi} &= \ket{0} - \ket{1} \\ \nonumber \\
     \ket{z}\bra{w} &= \big(\ket{0} + \ket{1}\big)\bra{0} \\
                    &= \ket{0}\bra{0} + \ket{1}\bra{0} \\ \nonumber \\
     \ket{z}\bra{w}\ket{\phi}
                    &= \big(\ket{0}\bra{0} + \ket{1}\bra{0}\big)\big(\ket{0} - \ket{1}\big) \\
                    &= \ket{0}\inner{0}{0} + \ket{1}\inner{0}{0} + \ket{1}\inner{0}{1} + \ket{1}\inner{0}{1} \\
                    &= \ket{0} + \ket{1}
  \end{align}
</div>
